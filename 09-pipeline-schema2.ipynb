{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same pipeline as in `07` and `08`, just putting it into a different database schema (`schema2.sql`) and running it on a computer where memory does not matter that much. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "import logging\n",
    "from typing import List\n",
    "from itertools import combinations, chain, compress\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"09-pipeline-schema2.log\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_string = \"mysql://bukanuser@localhost/bukan?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_query(query: str):\n",
    "    engine = create_engine(engine_string, convert_unicode=True)\n",
    "    with engine.connect() as conn:\n",
    "        results = conn.execute(text(query)).fetchall()\n",
    "    engine.dispose()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    \"\"\"From <https://github.com/kuk/log-progress>\"\"\"\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.read_csv(\"bukan-overview-extended.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>公開時期</th>\n",
       "      <th>オープンデータ分類</th>\n",
       "      <th>書名（統一書名）</th>\n",
       "      <th>刊・写</th>\n",
       "      <th>原本請求記号</th>\n",
       "      <th>刊年・書写年</th>\n",
       "      <th>（西暦）</th>\n",
       "      <th>冊数等</th>\n",
       "      <th>(単位)</th>\n",
       "      <th>Pages per Scan</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>TitleHiragana</th>\n",
       "      <th>TitleRomanji</th>\n",
       "      <th>NrImages</th>\n",
       "      <th>Estimate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>国文研書誌ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200018462</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>鎌倉武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１４９０－４</td>\n",
       "      <td>文政２</td>\n",
       "      <td>1819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Portrait</td>\n",
       "      <td>かまくらぶかん</td>\n",
       "      <td>kamakurabukan</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200018466</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>応仁武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１４９０－７</td>\n",
       "      <td>天保１５</td>\n",
       "      <td>1844</td>\n",
       "      <td>2.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Portrait</td>\n",
       "      <td>おうにんぶかん</td>\n",
       "      <td>ōninbukan</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200018476</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>応仁武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１４９０－８</td>\n",
       "      <td>弘化３</td>\n",
       "      <td>1846</td>\n",
       "      <td>3.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Portrait</td>\n",
       "      <td>おうにんぶかん</td>\n",
       "      <td>ōninbukan</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200018713</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>本朝武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－５３</td>\n",
       "      <td>貞享３</td>\n",
       "      <td>1686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>ほんちょうぶかん</td>\n",
       "      <td>honchōbukan</td>\n",
       "      <td>75</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200018714</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>本朝武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－５４</td>\n",
       "      <td>貞享３</td>\n",
       "      <td>1686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>ほんちょうぶかん</td>\n",
       "      <td>honchōbukan</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019654</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>懐宝御国分略武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－３１８</td>\n",
       "      <td>慶応４</td>\n",
       "      <td>1868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>かいほうおくにわけりゃくぶかん</td>\n",
       "      <td>kaihō okuniwake ryakubukan</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019661</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>昇栄武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－３２４</td>\n",
       "      <td>嘉永６</td>\n",
       "      <td>1853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>しょうえいぶかん</td>\n",
       "      <td>shōeibukan</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019662</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>昇栄武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－３２５</td>\n",
       "      <td>安政３</td>\n",
       "      <td>1856</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>しょうえいぶかん</td>\n",
       "      <td>shōeibukan</td>\n",
       "      <td>125</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019666</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>昇栄武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>ＭＹ－１２０１－３２７</td>\n",
       "      <td>元治１</td>\n",
       "      <td>1864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Landscape</td>\n",
       "      <td>しょうえいぶかん</td>\n",
       "      <td>shōeibukan</td>\n",
       "      <td>128</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200019756</th>\n",
       "      <td>H29.12</td>\n",
       "      <td>政治・法制</td>\n",
       "      <td>慶応武鑑</td>\n",
       "      <td>刊</td>\n",
       "      <td>９６－５５４－１～４</td>\n",
       "      <td>慶応３</td>\n",
       "      <td>1867</td>\n",
       "      <td>4.0</td>\n",
       "      <td>冊</td>\n",
       "      <td>2</td>\n",
       "      <td>Portrait</td>\n",
       "      <td>けいおうぶかん</td>\n",
       "      <td>keiōbukan</td>\n",
       "      <td>551</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             公開時期 オープンデータ分類  書名（統一書名） 刊・写       原本請求記号 刊年・書写年  （西暦）  冊数等 (単位)  \\\n",
       "国文研書誌ID                                                                         \n",
       "200018462  H29.12     政治・法制      鎌倉武鑑   刊    ＭＹ－１４９０－４    文政２  1819  1.0    冊   \n",
       "200018466  H29.12     政治・法制      応仁武鑑   刊    ＭＹ－１４９０－７   天保１５  1844  2.0    冊   \n",
       "200018476  H29.12     政治・法制      応仁武鑑   刊    ＭＹ－１４９０－８    弘化３  1846  3.0    冊   \n",
       "200018713  H29.12     政治・法制      本朝武鑑   刊   ＭＹ－１２０１－５３    貞享３  1686  1.0    冊   \n",
       "200018714  H29.12     政治・法制      本朝武鑑   刊   ＭＹ－１２０１－５４    貞享３  1686  1.0    冊   \n",
       "...           ...       ...       ...  ..          ...    ...   ...  ...  ...   \n",
       "200019654  H29.12     政治・法制  懐宝御国分略武鑑   刊  ＭＹ－１２０１－３１８    慶応４  1868  1.0    冊   \n",
       "200019661  H29.12     政治・法制      昇栄武鑑   刊  ＭＹ－１２０１－３２４    嘉永６  1853  1.0    冊   \n",
       "200019662  H29.12     政治・法制      昇栄武鑑   刊  ＭＹ－１２０１－３２５    安政３  1856  1.0    冊   \n",
       "200019666  H29.12     政治・法制      昇栄武鑑   刊  ＭＹ－１２０１－３２７    元治１  1864  1.0    冊   \n",
       "200019756  H29.12     政治・法制      慶応武鑑   刊   ９６－５５４－１～４    慶応３  1867  4.0    冊   \n",
       "\n",
       "           Pages per Scan     Aspect    TitleHiragana  \\\n",
       "国文研書誌ID                                                 \n",
       "200018462               2   Portrait          かまくらぶかん   \n",
       "200018466               2   Portrait          おうにんぶかん   \n",
       "200018476               2   Portrait          おうにんぶかん   \n",
       "200018713               2  Landscape         ほんちょうぶかん   \n",
       "200018714               2  Landscape         ほんちょうぶかん   \n",
       "...                   ...        ...              ...   \n",
       "200019654               2  Landscape  かいほうおくにわけりゃくぶかん   \n",
       "200019661               2  Landscape         しょうえいぶかん   \n",
       "200019662               2  Landscape         しょうえいぶかん   \n",
       "200019666               2  Landscape         しょうえいぶかん   \n",
       "200019756               2   Portrait          けいおうぶかん   \n",
       "\n",
       "                         TitleRomanji  NrImages  Estimate  \n",
       "国文研書誌ID                                                    \n",
       "200018462               kamakurabukan        69     False  \n",
       "200018466                   ōninbukan        78     False  \n",
       "200018476                   ōninbukan       100     False  \n",
       "200018713                 honchōbukan        75      True  \n",
       "200018714                 honchōbukan        94     False  \n",
       "...                               ...       ...       ...  \n",
       "200019654  kaihō okuniwake ryakubukan        24     False  \n",
       "200019661                  shōeibukan       124     False  \n",
       "200019662                  shōeibukan       125     False  \n",
       "200019666                  shōeibukan       128     False  \n",
       "200019756                   keiōbukan       551     False  \n",
       "\n",
       "[351 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting metadata into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = overview.groupby([\"書名（統一書名）\", \"TitleHiragana\", \"TitleRomanji\"]).mean()\n",
    "title = title.index.to_frame(index=False)\n",
    "title.columns = [\"kanji\", \"hiragana\", \"romanji\"]\n",
    "title.index = pd.RangeIndex(1,45)\n",
    "title.index.name = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string)\n",
    "#title.to_sql(\"title\", engine, if_exists=\"append\")\n",
    "title = pd.read_sql_table(\"title\", engine, index_col=\"id\")\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_kanji_to_id = {kanji:id for id, kanji in title[\"kanji\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "released = overview[\"公開時期\"].apply(lambda x: date(2017, 12, 1) if x == \"H29.12\" else date(2015, 11, 1))\n",
    "title_id = overview[\"書名（統一書名）\"].apply(lambda x: title_kanji_to_id[x])\n",
    "original_id = overview[\"原本請求記号\"]\n",
    "era_name = overview[\"刊年・書写年\"].apply(lambda x: x[:2])\n",
    "era_year = overview[\"刊年・書写年\"].apply(lambda x: int(x[2:]))\n",
    "year = overview[\"（西暦）\"]\n",
    "estimate = overview[\"Estimate\"]\n",
    "nr_books = overview[\"冊数等\"]\n",
    "pages_per_scan = overview[\"Pages per Scan\"]\n",
    "aspect = overview[\"Aspect\"].apply(lambda x: x[0])\n",
    "nr_scans = overview[\"NrImages\"]\n",
    "book = pd.concat([released, title_id, original_id, era_name, era_year, year, estimate, nr_books, pages_per_scan, aspect, nr_scans], axis=1)\n",
    "book.columns = [\"released\", \"title_id\", \"original_id\", \"era_name\", \"era_year\", \"year\", \"estimate\", \"nr_books\", \"pages_per_scan\", \"aspect\", \"nr_scans\"]\n",
    "book.index.name = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string)\n",
    "#book.to_sql(\"book\", engine, if_exists=\"append\")\n",
    "book = pd.read_sql_table(\"book\", engine, index_col=\"id\")\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string)\n",
    "index = 1\n",
    "for book_id, book_metadata in overview.iterrows():\n",
    "    break\n",
    "    pages_per_scan = book_metadata[\"Pages per Scan\"]\n",
    "    image_paths = glob(f\"data/{book_id}/image/*.jpg\")\n",
    "    image_filenames = [os.path.splitext(os.path.split(path)[1]) for path in image_paths]\n",
    "    if pages_per_scan == 1:\n",
    "        image_filenames = [name + \"-w\" + ext for name, ext in  image_filenames]\n",
    "        image_filenames.sort()\n",
    "    else:\n",
    "        image_filenames = [name + \"-l\" + ext for name, ext in  image_filenames] + [name + \"-r\" + ext for name, ext in  image_filenames]\n",
    "        image_filenames.sort()\n",
    "    page = pd.DataFrame(image_filenames, columns=[\"filename\"]).assign(book_id=book_id)\n",
    "    page[\"lr\"] = page[\"filename\"].apply(lambda x: x[16])\n",
    "    page[\"page\"] = page[\"filename\"].apply(lambda x: int(x[10:15]))\n",
    "    page.index = pd.RangeIndex(index, index + len(page), name=\"id\")\n",
    "    index = index + len(page)\n",
    "    page.to_sql(\"page\", engine, if_exists=\"append\")\n",
    "page = pd.read_sql_table(\"page\", engine, index_col=\"id\")\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I need to do now per image is:\n",
    "\n",
    "1. Read, greyscale and crop all images\n",
    "2. Split right/left page if necessary\n",
    "3. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(Enum):\n",
    "    \"\"\"Japanese reading order is from right to left.\"\"\"\n",
    "    whole =  b\"w\"\n",
    "    left =   b\"l\"\n",
    "    right  = b\"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    target_height = 660\n",
    "    target_width = 990\n",
    "    height, width = img.shape\n",
    "    x1 = (width - target_width) // 2\n",
    "    y1 = (height - target_height) // 2\n",
    "    x2 = x1 + target_width\n",
    "    y2 = y1 + target_height\n",
    "    return img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv.imread(path, flags=cv.IMREAD_REDUCED_GRAYSCALE_4)\n",
    "    img = crop_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(img):\n",
    "    height, width = img.shape\n",
    "    assert width == 990\n",
    "    half_width = width // 2\n",
    "    return img[:, :half_width], img[:, half_width:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_dataframe(keypoints: List[cv.KeyPoint], descriptors: np.ndarray, page_id: int):\n",
    "    df = pd.DataFrame([(kp.pt[0], kp.pt[1], kp.size, kp.angle, kp.response, kp.octave, kp.class_id) for kp in keypoints],\n",
    "                      columns=[\"x\", \"y\", \"size\", \"angle\", \"response\", \"octave\", \"class_id\"])\n",
    "    df = (df\n",
    "          .assign(descriptor=[bytes(descriptors[i,:]) for i in range(descriptors.shape[0])])\n",
    "          .assign(feature_nr=df.index)\n",
    "          .assign(page_id=len(df)*[page_id]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6205bc872d4fbea0b3410443eeac3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=150538)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector = cv.AKAZE_create(cv.AKAZE_DESCRIPTOR_MLDB_UPRIGHT, descriptor_size=0, threshold=0.005)\n",
    "index = 1\n",
    "all_features = []\n",
    "for page_id, page_metadata in log_progress(page.iterrows(), every=100, size=len(page), name=\"Page\"):\n",
    "    break\n",
    "    image_path = \"data/{}/image/{}\".format(page_metadata[\"book_id\"], page_metadata[\"filename\"])\n",
    "    image_path = image_path[:-6] + image_path[-4:]\n",
    "    image = read_image(image_path)\n",
    "    if page_metadata[\"lr\"] == b\"l\":\n",
    "        image, _ = split_image(image)\n",
    "    elif page_metadata[\"lr\"] == b\"r\":\n",
    "        _, image = split_image(image)\n",
    "    keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "    if len(keypoints) > 0:\n",
    "        feature_df = features_to_dataframe(keypoints, descriptors, page_id)\n",
    "        feature_df.index = pd.RangeIndex(index, index + len(feature_df), name=\"id\")\n",
    "        feature_df.to_sql(\"feature\", engine, if_exists=\"append\")\n",
    "        index = index + len(feature_df)\n",
    "        all_features.append(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = pd.concat(all_features)\n",
    "#feature = pd.read_parquet(\"feature.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Feature Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to get all all book combinations as well as a fixed page offset. For each combination I need to run the full pipeline:\n",
    "\n",
    "1. Find matching features\n",
    "2. Filter features by their position\n",
    "3. Compute the homography\n",
    "4. Select features using the homography mask\n",
    "4. **Don't threshold the features**\n",
    "5. Save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_radius = 4\n",
    "book_pairs_lr = set()\n",
    "book_pairs_w = set()\n",
    "for i in range(book.index.size):\n",
    "    lower_bound = i - book_radius\n",
    "    lower_bound = 0 if lower_bound < 0 else lower_bound\n",
    "    upper_bound = i + book_radius + 1\n",
    "    upper_bound = book.index.size if upper_bound >= book.index.size else upper_bound\n",
    "    for j in range(lower_bound, upper_bound):\n",
    "        if j != i:\n",
    "            book1 = book.iloc[i]\n",
    "            book2 = book.iloc[j]\n",
    "            if book1[\"pages_per_scan\"] == book2[\"pages_per_scan\"]:\n",
    "                if book1[\"pages_per_scan\"] == 2: \n",
    "                    book_pairs_lr.add(frozenset([book1.name, book2.name]))\n",
    "                else:  # This should be 1\n",
    "                    assert book1[\"pages_per_scan\"] == 1\n",
    "                    book_pairs_w.add(frozenset([book1.name, book2.name]))\n",
    "book_pairs_lr = [sorted([book1, book2]) for book1, book2 in book_pairs_lr]\n",
    "book_pairs_lr.sort()\n",
    "book_pairs_w = [sorted([book1, book2]) for book1, book2 in book_pairs_w]\n",
    "book_pairs_w.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string, convert_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages(left, right, engine):\n",
    "    page_radius = 8\n",
    "    rmax_page = right[-1][1]\n",
    "    right_inv_dict = {rpage:rid for rid, rpage in right}\n",
    "    page_id_pairs = []\n",
    "    for lid, lpage in left:\n",
    "        lower_bound = lpage - page_radius\n",
    "        lower_bound = 1 if lower_bound < 1 else lower_bound\n",
    "        upper_bound = lpage + page_radius\n",
    "        upper_bound = rmax_page if upper_bound > rmax_page else upper_bound\n",
    "        for rpage in range(lower_bound, upper_bound + 1):\n",
    "            page_id_pairs.append((lid, right_inv_dict[rpage]))\n",
    "    return page_id_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_book_pairs(pairs, lr: Page, engine):\n",
    "    statement = text(\"SELECT id, page FROM page WHERE book_id = :book AND lr = :lr\")\n",
    "    all_pairs = []\n",
    "    for book1, book2 in pairs:\n",
    "        with engine.connect() as conn:\n",
    "            pages1 = conn.execute(statement, {\"book\": book1, \"lr\": lr.value}).fetchall()\n",
    "            pages2 = conn.execute(statement, {\"book\": book2, \"lr\": lr.value}).fetchall()\n",
    "            page_id_pairs = process_pages(pages1, pages2, engine)\n",
    "            all_pairs.extend(page_id_pairs)\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id_pairs = process_book_pairs(book_pairs_lr, Page.left, engine)\n",
    "page_id_pairs.extend(process_book_pairs(book_pairs_lr, Page.right, engine))\n",
    "page_id_pairs.extend(process_book_pairs(book_pairs_w, Page.whole, engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "wradius = 100.\n",
    "hradius = 100.\n",
    "def is_near(points_pair):\n",
    "    (x1, y1), (x2, y2) = points_pair\n",
    "    return ((x1 - wradius) <= x2 <= (x1 + wradius)) and ((y1 - hradius) <= y2 <= (y1 + hradius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_desc_array(feature_tuple):\n",
    "    result = np.empty((len(feature_tuple), 61), dtype=np.uint8)\n",
    "    for i, val in enumerate(feature_tuple):\n",
    "        result[i,:] = np.frombuffer(val[-1], dtype=np.uint8)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ae1f12e9ab4cf08b1eb4f125f1950b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=9105974)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matcher = cv.BFMatcher_create(normType=cv.NORM_HAMMING)\n",
    "max_match_distance = 100\n",
    "statement = text(\"SELECT id, x, y, descriptor FROM feature WHERE page_id = :page ORDER BY feature_nr\")\n",
    "dmatch_id = 1\n",
    "with engine.connect() as conn:\n",
    "    for ppid, (page1, page2) in log_progress(enumerate(page_id_pairs), every=1000, size=len(page_id_pairs), name=\"Pair\"):\n",
    "        break\n",
    "        features1 = conn.execute(statement, {\"page\": page1}).fetchall()\n",
    "        features2 = conn.execute(statement, {\"page\": page2}).fetchall()\n",
    "        desc1 = create_desc_array(features1)\n",
    "        desc2 = create_desc_array(features2)\n",
    "        # 1. find_matches\n",
    "        matches = matcher.radiusMatch(desc1, desc2, max_match_distance)\n",
    "        matches = list(chain.from_iterable(matches))\n",
    "        if len(matches) == 0:\n",
    "            pagepair = (ppid+1, page1, page2, 0)\n",
    "            conn.execute(text(\"INSERT INTO pagepair (id, first_page, second_page, nr_matches) VALUES {}\".format(str(pagepair))))\n",
    "            continue\n",
    "        # 2. select_keypoints\n",
    "        points1 = np.empty((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.empty((len(matches), 2), dtype=np.float32)\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = features1[match.queryIdx][1:3]\n",
    "            points2[i, :] = features2[match.trainIdx][1:3]\n",
    "        zipped_points = zip(points1, points2)\n",
    "        points_mask = np.fromiter(map(is_near, zipped_points), dtype=np.bool, count=len(matches))\n",
    "        points1 = points1[points_mask]\n",
    "        points2 = points2[points_mask]\n",
    "        assert points1.shape == points2.shape\n",
    "        if points1.size == 0:\n",
    "            pagepair = (ppid+1, page1, page2, 0)\n",
    "            conn.execute(text(\"INSERT INTO pagepair (id, first_page, second_page, nr_matches) VALUES {}\".format(str(pagepair))))\n",
    "            continue\n",
    "        # 3. compute_homography\n",
    "        h, h_mask = cv.findHomography(points1, points2, cv.RANSAC)\n",
    "        if not h_mask.sum() > 0:\n",
    "            pagepair = (ppid+1, page1, page2, 0)\n",
    "            conn.execute(text(\"INSERT INTO pagepair (id, first_page, second_page, nr_matches) VALUES {}\".format(str(pagepair))))\n",
    "            continue\n",
    "        # 4. filter_bad_homographies\n",
    "        is_bad_h = np.any(np.isnan(h)) or np.any(np.absolute(h[2,:2]) > 0.001)\n",
    "        if is_bad_h:\n",
    "            if np.any(np.isinf(h)):\n",
    "                pagepair = (ppid+1, page1, page2, 0)\n",
    "                conn.execute(text(\"INSERT INTO pagepair (id, first_page, second_page, nr_matches) VALUES {}\".format(str(pagepair))))\n",
    "            else:\n",
    "                pagepair = (ppid+1, page1, page2, 0, h[0,0], h[0,1], h[0,2], h[1,0], h[1,1], h[1,2], h[2,0], h[2,1], h[2,2])\n",
    "                conn.execute(text(\"INSERT INTO pagepair VALUES {}\".format(str(pagepair))))\n",
    "            continue\n",
    "        # 5. chose_relevant_matches\n",
    "        h_mask = np.squeeze(h_mask).astype(np.bool)\n",
    "        j = 0\n",
    "        for i in range(len(points_mask)):\n",
    "            if points_mask[i]:\n",
    "                if not h_mask[j]:\n",
    "                    points_mask[i] = False\n",
    "                j += 1\n",
    "        assert points_mask.sum() == h_mask.sum()\n",
    "        relevant_matches = list(compress(matches, points_mask))\n",
    "        relevant_pairs = [(idx, features1[match.queryIdx][0], features2[match.trainIdx][0])\n",
    "                          for idx, match in zip(range(dmatch_id, dmatch_id+len(relevant_matches)), relevant_matches)]\n",
    "        dmatch_id = dmatch_id + len(relevant_matches)\n",
    "        # 6. Insert in database\n",
    "        pagepair = (ppid+1, page1, page2, len(relevant_pairs), h[0,0], h[0,1], h[0,2], h[1,0], h[1,1], h[1,2], h[2,0], h[2,1], h[2,2])\n",
    "        conn.execute(text(\"INSERT INTO pagepair VALUES {}\".format(str(pagepair))))\n",
    "        conn.execute(text(\"INSERT INTO dmatch VALUES {}\".format(\", \".join(str(x) for x in relevant_pairs))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
