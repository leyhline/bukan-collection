\documentclass{ltjarticle}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{authblk}

\renewcommand{\figurename}{Figure}

\title{Computer Vision for Comparison and Display of Different Versions of Woodblock-printed Books and its Application to Bukan}

\author[1,3]{Thomas Leyh}
\author[2,3]{Asanobu Kitamoto}
\affil[1]{University of Freiburg}
\affil[2]{ROIS-DS Center for Open Data in the Humanities}
\affil[3]{National Institute of Informatics}

\date{July 7th, 2020}

\usepackage[backend=biber,style=alphabetic,sorting=nyt]{biblatex}
\addbibresource{references.bib}

\begin{document}

\maketitle

\section{Introduction}

In the Digital Humanitites, one of the basic goals is to make comparison possible. This allows for easier classification and enables search inside a dataset. On the one hand, there are numerous text-based methods along with large corpera of literary text. On the other hand, the emerge of large image datasets directly leads to methods from Computer Vision, skipping the necessarity of text transcriptions. This paper describes the technical development of this approach as well as its application to Bukan, a special type of Japanese woodblock printed books.

The authors have been approaching this problem in the past. \cite{kitamoto2018} proposed the concept of “differential reading” for visual comparison. Furthermore, \cite{hakim2019} proposed “visual named-entity recognition” for identifying family crests, using them for a page-by-page matching across different versions. This paper is a follow-up of these works and proposes a keypoint-based method for the page-by-page matching, additionally yielding an option for highlighting differences. 

\section{Dataset}

This work is mainly concerned with extracting information from a specific type of book: 武鑑---Bukan. These are historic Japanese books from the Edo period (1603-1868). Serving as \emph{unofficial} directories of Japanese bureaucrats, they include a wealth of information about regional characteristics such as persons, families and other key factors. See figure~\ref{fig:shuugyokubukan006} for an example. These books were created with woodblock-printing. Because the same woodblock has been reused for many versions of the book---sometimes with minor modifications---visual comparison can reveal which part of the woodblock was modified or has degraded.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{200019649_00006}
    \caption[Shūgyoku Bukan (袖玉武鑑), page 6]{Shūgyoku Bukan (袖玉武鑑) from 1867, page 6; showing names, descriptions, family crests and procession items. Especially interesting are the blank areas on the right, because in other edition they contain text.}
    \label{fig:shuugyokubukan006}
\end{figure}

ROIS-DS Center for Open Data in the Humanities and the National Institute of Japanese Literature are offering 381 of these Bukan as open data \cite{pmjt} \cite{codh2018bukan}. The original images have a width of $5616$ and height of $3744$ pixels. Under the assumption that this task (1) does not require this level of detail, (2) does not require information about color and (3) only compares the actual pages, not the surrounding area, basic image transformations are applied. All scans are resized by $25\%$, converted to grayscale and finally cropped, resulting in an image shape of $990 \times 660$ pixels. If there are two book pages per scan, they were additionally split at their horizontal center.

\section{Method}

Using an approach based on Computer Vision, two techniques were applied:

\begin{enumerate}
    \item \emph{Keypoint Detection and Matching} for finding the same features in different images.
    \item \emph{Projective Transformations} for comparing two different images regardless of their original orientation.
\end{enumerate}

We used the \emph{OpenCV} software library.\cite{opencv_library}

\subsection{Keypoint Matching}

\emph{Keypoint Detection}\cite[Ch.4]{szeliski2010computer} is abount finding points of interest in an image that are most noticeable and give a unique description of the local area surrounding them. Computer Vision research produced various kinds of keypoints, most prominently \emph{SIFT}.\cite{lowe2004sift} For evaluating the performance of these algorithms, 12 prints of the \emph{Shūchin Bukan} (袖珍武鑑) were manually annotated, in total around 1800 pages, holding information about the position of matching pages.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{keypoint-evaluation.pdf}
    \caption[Scatterplot of keypoint matching performance]{Scatterplot for evaluating keypoint matching performance by plotting the value of precision-recall-intersection on the y-axis. All points above the gray horizontal line at $0.8$ are considered “good enough” by the authors. Even though this metric discards much information, there is a general tendency in favor of AKAZE UPRIGHT discernible.}
    \label{fig:keypoint-evaluation}
\end{figure}

Using these annotations, six keypoint algorithms were empirically evaluated\footnote{ORB\cite{rublee2011orb}, AKAZE\cite{alcantarilla2011fast}, AKAZE without rotational invariance (UPRIGHT), BRISK\cite{leutenegger2011brisk}, SIFT, SURF\cite{bay2006surf}} by trying to match over all possible combinations. At this point, AKAZE UPRIGHT had the best performance with an average precision and recall of $0.8$. See figure~\ref{fig:keypoint-evaluation} for further details.

\subsection{Projective Transformations}

For images, a Projective Transformation (or \emph{Homography}) is a matrix $\mathbf{H} \in \mathbb{R}^{3 \times 3}$ for transforming homogeneous coordinates $\mathbf{H}\vec{x} = \vec{y}$ ($\vec{x}$ and $\vec{y}$ are interpolated pixel coordinates). This operation results in linear transformations like translation and rotation, but also changes in perspective.\cite{marschner2015fundamentals} For finding such a transformation from matching keypoints, the heuristic \emph{Random Sample Consensus} (RANSAC) algorithm is commonly used.\cite{fischler1981random} Basically, a random subset of matching keypoints is chosen, using this to compute a transformation and calculating an error metric. By iteratively using different random subsets, eventually the transformation with the smallest error is picked.

\begin{figure}[p]
    \centering
    \includegraphics[width=0.9\textwidth]{ransac-performance.png}
    \caption[RANSAC performance]{Precision and recall (y-axis) by threshold on the number of matching keypoints (y-axis), just showing values above $0.95$. The recall is always on an excellent level while the threshold should not be too low because precision will collapse otherwise.}
    \label{fig:ransac}
\end{figure}

The benefit is twofold: First, the algorithm implicitly uses spartial information of the matching image candidates to filter out false positives, thus greatly boosting the matching performance. Precision and recall are close to their maximum of $1.0$, see figure~\ref{fig:ransac}. Secondly, it directly yields the transformation matrix $\mathbf{H}$, enabling the creation of image overlays for visualizing the differences. Looking at the perspective components of $\mathbf{H}$, additional filtering is done\footnote{By asserting that $|\mathbf{H}_{1,3}| \leq 0.001$ and $|\mathbf{H}_{2,3}| \leq 0.001$.}, removing even more of the remaining false positives.

\section{Discussion}

This two-step-pipeline archieves high precision and recall when looking for similar pages of different book editions. Performance seems to be robust with respect to most parameters. A final threshold on the number of matching keypoints decides if a pair of pages is classified as match. Assuming semi-automatic application with a human assessing the results, recall is of higher importance, thus a low threshold of around $40$ is recommended. Besides, this value might be used as score for estimating the similarity of a page pair.

Computing keypoints is possible in linear time with respect to the number of given images. Finding matches takes quadratic time without prior assumptions since image pairs are processed. Speeding this up should be possible by using techniques from robotics, since the situation is similar: A small number of keypoints from image sensors is for querying a large database of keypoints in memory.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{200019646_00006}
    \caption[Visualizing page differences]{Visualizing page differences between two prints of the Shūchin Bukan (袖珍武鑑) from 1867. Differences are indicated by \textcolor{blue}{blueish} and \textcolor{red}{reddish} coloring.}
    \label{fig:page-compare}
\end{figure}

This can serve as base for building a comparison browser: A scholar in the humanities and literature can browse a book while getting information about similar pages. Differences between pages can be visualized at any time, similar to figure~\ref{fig:page-compare}. This will hopefully lessen her burden when examining numerous pages, since small, abstract differences are often hard to discern for the human eye. 

\section{Conclusion}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque vehicula vitae ligula vitae pellentesque. Aliquam placerat commodo urna ac luctus. Nullam aliquam purus diam, vel ornare quam tempor at. In ut est tempus, finibus dolor sed, semper tortor. Cras pharetra mollis nisl. Praesent a nisl tellus. In faucibus, ipsum sit amet varius porta, leo nibh ultrices nisl, a porta nunc nisi commodo tortor. Aenean efficitur viverra urna. Mauris scelerisque orci faucibus, gravida neque non, rutrum enim.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent neque ex, ultricies vulputate magna eu, dignissim consequat ligula. Etiam faucibus bibendum dolor, at ultricies. 

\printbibliography

\end{document}
