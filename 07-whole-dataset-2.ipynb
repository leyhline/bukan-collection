{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "from itertools import chain, combinations\n",
    "import time\n",
    "from collections.abc import Iterable\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.features import show_image\n",
    "from helpers.pipeline import applymapi, find_matches, select_keypoints, compute_homography_and_mask, sum_homography_mask, filter_out_bad_homographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    \"\"\"From <https://github.com/kuk/log-progress>\"\"\"\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and filter metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.read_csv(\"bukan-overview.csv\", index_col=1, dtype={\n",
    "    \"国文研書誌ID\": str, \"冊数等\": str\n",
    "})\n",
    "overview = overview[overview[\"K?\"] == \"x\"].drop(\"K?\", 1).drop(\"Comment\", 1).astype({\"Pages per Scan\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists_of_images(overview_df):\n",
    "    images_per_book = []\n",
    "    for book_id in overview_df.index:\n",
    "        book_id_path = os.path.join(\"data\", str(book_id))\n",
    "        assert os.path.exists(book_id_path)\n",
    "        assert os.path.isdir(book_id_path)\n",
    "        book_id_image_path = os.path.join(book_id_path, \"image\")\n",
    "        assert os.path.exists(book_id_image_path)\n",
    "        assert os.path.isdir(book_id_image_path)\n",
    "        images = glob(f\"{book_id_image_path}/*.jpg\")\n",
    "        images_per_book.append(images)\n",
    "    return images_per_book\n",
    "overview = overview.assign(Images=get_lists_of_images(overview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = overview.assign(NrImages=overview[\"Images\"].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_filtered = overview.drop([\"公開時期\", \"オープンデータ分類\", \"刊・写\", \"原本請求記号\", \"刊年・書写年\", \"（西暦）\", \"冊数等\", \"(単位)\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(Enum):\n",
    "    \"\"\"Japanese reading order is from right to left.\"\"\"\n",
    "    unknown = 0\n",
    "    right   = 1\n",
    "    left    = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_bukan = overview_filtered.groupby([\"書名（統一書名）\", \"Pages per Scan\"]).count()\n",
    "count_by_bukan = count_by_bukan[count_by_bukan[\"Images\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping two strange rows where the page format doesn't fit the rest of the Bukan editions\n",
    "overview_filtered = overview_filtered.mask(\n",
    "    (overview_filtered[\"書名（統一書名）\"] == \"有司武鑑\") &\n",
    "    (overview_filtered[\"Pages per Scan\"] == 2)).dropna().astype({\"NrImages\":int, \"Pages per Scan\":int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_bukan_count = overview_filtered.groupby([\"書名（統一書名）\"]).count()[\"Images\"]\n",
    "overview_bukan_count = overview_bukan_count[overview_bukan_count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are Bukan titles with more than one edition:\n",
    "useful_bukan_titles = set(overview_bukan_count.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_overview = overview_filtered[overview_filtered[\"書名（統一書名）\"].isin(useful_bukan_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>書名（統一書名）</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>万世武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>149.333333</td>\n",
       "      <td>49.692387</td>\n",
       "      <td>92.0</td>\n",
       "      <td>134.00</td>\n",
       "      <td>176.0</td>\n",
       "      <td>178.00</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>享保武鑑</th>\n",
       "      <td>6.0</td>\n",
       "      <td>251.666667</td>\n",
       "      <td>76.912071</td>\n",
       "      <td>96.0</td>\n",
       "      <td>271.25</td>\n",
       "      <td>279.5</td>\n",
       "      <td>285.50</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>享和武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>463.666667</td>\n",
       "      <td>9.018500</td>\n",
       "      <td>455.0</td>\n",
       "      <td>459.00</td>\n",
       "      <td>463.0</td>\n",
       "      <td>468.00</td>\n",
       "      <td>473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>元文武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>286.0</td>\n",
       "      <td>287.50</td>\n",
       "      <td>289.0</td>\n",
       "      <td>290.50</td>\n",
       "      <td>292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>元禄武鑑大全</th>\n",
       "      <td>4.0</td>\n",
       "      <td>57.250000</td>\n",
       "      <td>11.056672</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>61.25</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>大成武鑑</th>\n",
       "      <td>24.0</td>\n",
       "      <td>568.250000</td>\n",
       "      <td>151.016627</td>\n",
       "      <td>81.0</td>\n",
       "      <td>574.75</td>\n",
       "      <td>643.5</td>\n",
       "      <td>648.25</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天保武鑑</th>\n",
       "      <td>15.0</td>\n",
       "      <td>501.533333</td>\n",
       "      <td>93.670293</td>\n",
       "      <td>228.0</td>\n",
       "      <td>524.00</td>\n",
       "      <td>527.0</td>\n",
       "      <td>547.00</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天明武鑑</th>\n",
       "      <td>7.0</td>\n",
       "      <td>379.571429</td>\n",
       "      <td>146.632031</td>\n",
       "      <td>51.0</td>\n",
       "      <td>405.00</td>\n",
       "      <td>435.0</td>\n",
       "      <td>449.00</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>太平武鑑</th>\n",
       "      <td>8.0</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>21.552262</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.50</td>\n",
       "      <td>52.0</td>\n",
       "      <td>73.25</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>安政武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>555.666667</td>\n",
       "      <td>6.429101</td>\n",
       "      <td>551.0</td>\n",
       "      <td>552.00</td>\n",
       "      <td>553.0</td>\n",
       "      <td>558.00</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>安永武鑑</th>\n",
       "      <td>9.0</td>\n",
       "      <td>273.666667</td>\n",
       "      <td>164.990909</td>\n",
       "      <td>46.0</td>\n",
       "      <td>109.00</td>\n",
       "      <td>292.0</td>\n",
       "      <td>433.00</td>\n",
       "      <td>435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>宝暦武鑑</th>\n",
       "      <td>6.0</td>\n",
       "      <td>309.500000</td>\n",
       "      <td>46.155173</td>\n",
       "      <td>279.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>291.5</td>\n",
       "      <td>311.75</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>宝永武鑑大成</th>\n",
       "      <td>2.0</td>\n",
       "      <td>178.500000</td>\n",
       "      <td>7.778175</td>\n",
       "      <td>173.0</td>\n",
       "      <td>175.75</td>\n",
       "      <td>178.5</td>\n",
       "      <td>181.25</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>寛政武鑑</th>\n",
       "      <td>8.0</td>\n",
       "      <td>447.750000</td>\n",
       "      <td>24.995714</td>\n",
       "      <td>388.0</td>\n",
       "      <td>450.50</td>\n",
       "      <td>453.5</td>\n",
       "      <td>461.00</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>弘化武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>546.666667</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>545.0</td>\n",
       "      <td>546.00</td>\n",
       "      <td>547.0</td>\n",
       "      <td>547.50</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>御国分武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.5</td>\n",
       "      <td>28.75</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>応仁武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>15.556349</td>\n",
       "      <td>78.0</td>\n",
       "      <td>83.50</td>\n",
       "      <td>89.0</td>\n",
       "      <td>94.50</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>懐宝略武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文久武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>573.0</td>\n",
       "      <td>574.50</td>\n",
       "      <td>576.0</td>\n",
       "      <td>577.50</td>\n",
       "      <td>579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文化武鑑</th>\n",
       "      <td>16.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>85.833172</td>\n",
       "      <td>160.0</td>\n",
       "      <td>463.75</td>\n",
       "      <td>470.5</td>\n",
       "      <td>474.00</td>\n",
       "      <td>487.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文政武鑑</th>\n",
       "      <td>13.0</td>\n",
       "      <td>461.153846</td>\n",
       "      <td>97.375430</td>\n",
       "      <td>178.0</td>\n",
       "      <td>483.00</td>\n",
       "      <td>489.0</td>\n",
       "      <td>509.00</td>\n",
       "      <td>524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>昇栄武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>125.666667</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.50</td>\n",
       "      <td>125.0</td>\n",
       "      <td>126.50</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>明和武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>22.627417</td>\n",
       "      <td>363.0</td>\n",
       "      <td>371.00</td>\n",
       "      <td>379.0</td>\n",
       "      <td>387.00</td>\n",
       "      <td>395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有司武鑑</th>\n",
       "      <td>26.0</td>\n",
       "      <td>188.769231</td>\n",
       "      <td>6.326501</td>\n",
       "      <td>170.0</td>\n",
       "      <td>188.00</td>\n",
       "      <td>190.0</td>\n",
       "      <td>193.50</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>本朝武鑑</th>\n",
       "      <td>3.0</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>10.016653</td>\n",
       "      <td>75.0</td>\n",
       "      <td>77.00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>86.50</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>正徳武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.00</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>正統武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>97.580736</td>\n",
       "      <td>86.0</td>\n",
       "      <td>120.50</td>\n",
       "      <td>155.0</td>\n",
       "      <td>189.50</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>正風武鑑</th>\n",
       "      <td>2.0</td>\n",
       "      <td>224.500000</td>\n",
       "      <td>64.346717</td>\n",
       "      <td>179.0</td>\n",
       "      <td>201.75</td>\n",
       "      <td>224.5</td>\n",
       "      <td>247.25</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>袖玉武鑑</th>\n",
       "      <td>100.0</td>\n",
       "      <td>173.430000</td>\n",
       "      <td>16.107753</td>\n",
       "      <td>118.0</td>\n",
       "      <td>166.00</td>\n",
       "      <td>178.0</td>\n",
       "      <td>186.00</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>袖珍武鑑</th>\n",
       "      <td>56.0</td>\n",
       "      <td>152.928571</td>\n",
       "      <td>3.372925</td>\n",
       "      <td>144.0</td>\n",
       "      <td>152.00</td>\n",
       "      <td>153.5</td>\n",
       "      <td>156.00</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count        mean         std    min     25%    50%     75%    max\n",
       "書名（統一書名）                                                                    \n",
       "万世武鑑        3.0  149.333333   49.692387   92.0  134.00  176.0  178.00  180.0\n",
       "享保武鑑        6.0  251.666667   76.912071   96.0  271.25  279.5  285.50  299.0\n",
       "享和武鑑        3.0  463.666667    9.018500  455.0  459.00  463.0  468.00  473.0\n",
       "元文武鑑        2.0  289.000000    4.242641  286.0  287.50  289.0  290.50  292.0\n",
       "元禄武鑑大全      4.0   57.250000   11.056672   44.0   53.00   57.0   61.25   71.0\n",
       "大成武鑑       24.0  568.250000  151.016627   81.0  574.75  643.5  648.25  666.0\n",
       "天保武鑑       15.0  501.533333   93.670293  228.0  524.00  527.0  547.00  553.0\n",
       "天明武鑑        7.0  379.571429  146.632031   51.0  405.00  435.0  449.00  463.0\n",
       "太平武鑑        8.0   64.250000   21.552262   48.0   50.50   52.0   73.25  103.0\n",
       "安政武鑑        3.0  555.666667    6.429101  551.0  552.00  553.0  558.00  563.0\n",
       "安永武鑑        9.0  273.666667  164.990909   46.0  109.00  292.0  433.00  435.0\n",
       "宝暦武鑑        6.0  309.500000   46.155173  279.0  281.00  291.5  311.75  399.0\n",
       "宝永武鑑大成      2.0  178.500000    7.778175  173.0  175.75  178.5  181.25  184.0\n",
       "寛政武鑑        8.0  447.750000   24.995714  388.0  450.50  453.5  461.00  465.0\n",
       "弘化武鑑        3.0  546.666667    1.527525  545.0  546.00  547.0  547.50  548.0\n",
       "御国分武鑑       2.0   28.500000    0.707107   28.0   28.25   28.5   28.75   29.0\n",
       "応仁武鑑        2.0   89.000000   15.556349   78.0   83.50   89.0   94.50  100.0\n",
       "懐宝略武鑑       2.0   22.000000    0.000000   22.0   22.00   22.0   22.00   22.0\n",
       "文久武鑑        2.0  576.000000    4.242641  573.0  574.50  576.0  577.50  579.0\n",
       "文化武鑑       16.0  442.000000   85.833172  160.0  463.75  470.5  474.00  487.0\n",
       "文政武鑑       13.0  461.153846   97.375430  178.0  483.00  489.0  509.00  524.0\n",
       "昇栄武鑑        3.0  125.666667    2.081666  124.0  124.50  125.0  126.50  128.0\n",
       "明和武鑑        2.0  379.000000   22.627417  363.0  371.00  379.0  387.00  395.0\n",
       "有司武鑑       26.0  188.769231    6.326501  170.0  188.00  190.0  193.50  198.0\n",
       "本朝武鑑        3.0   82.666667   10.016653   75.0   77.00   79.0   86.50   94.0\n",
       "正徳武鑑        2.0  275.000000    0.000000  275.0  275.00  275.0  275.00  275.0\n",
       "正統武鑑        2.0  155.000000   97.580736   86.0  120.50  155.0  189.50  224.0\n",
       "正風武鑑        2.0  224.500000   64.346717  179.0  201.75  224.5  247.25  270.0\n",
       "袖玉武鑑      100.0  173.430000   16.107753  118.0  166.00  178.0  186.00  196.0\n",
       "袖珍武鑑       56.0  152.928571    3.372925  144.0  152.00  153.5  156.00  162.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_overview_by_title = final_overview.groupby([\"書名（統一書名）\"]).describe()[\"NrImages\"]\n",
    "final_overview_by_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find features (descriptors and keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    target_height = 660\n",
    "    target_width = 990\n",
    "    height, width = img.shape\n",
    "    x1 = (width - target_width) // 2\n",
    "    y1 = (height - target_height) // 2\n",
    "    x2 = x1 + target_width\n",
    "    y2 = y1 + target_height\n",
    "    return img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv.imread(path, flags=cv.IMREAD_REDUCED_GRAYSCALE_4)\n",
    "    img = crop_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(img):\n",
    "    height, width = img.shape\n",
    "    assert width == 990\n",
    "    return img[:,:width//2], img[:,width//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_images_and_features(overview_df):\n",
    "    \"\"\"\n",
    "    In theory, this large and ugly function only needs to run once.\n",
    "    Here, features (descriptors and keypoints) for each image are\n",
    "    calculated on a greyscaled and cropped input image.\n",
    "    \n",
    "    The image and the features are saved to disk. The latter are stored\n",
    "    in Apache Parquet format.\n",
    "    \"\"\"\n",
    "    detector = cv.AKAZE_create(cv.AKAZE_DESCRIPTOR_MLDB_UPRIGHT, descriptor_size=0, threshold=0.005)\n",
    "    for book_id, book_metadata in log_progress(overview_df.iterrows(), every=1, size=len(overview_df)):\n",
    "        os.makedirs(f\"data/grey/{str(book_id)}/image\", exist_ok=True)\n",
    "        pps = book_metadata[\"Pages per Scan\"]\n",
    "        image_paths = book_metadata[\"Images\"]\n",
    "        keypoints_list = []\n",
    "        descriptors_list = []\n",
    "        index_col_page = []\n",
    "        index_col_type = []\n",
    "        index_col_i = []\n",
    "        \n",
    "        # This part is for processing the images and saving them,\n",
    "        # as well as for calculating the features\n",
    "        for page_i, image_path in enumerate(image_paths):\n",
    "            image = read_image(image_path)\n",
    "            assert (pps == 1) or (pps == 2)\n",
    "            if pps == 1:  # It's just one page\n",
    "                assert cv.imwrite(f\"data/grey/{str(book_id)}/image/{str(book_id)}_{page_i+1:0>5}_{Page.unknown.value}.jpg\",\n",
    "                                  image, [cv.IMWRITE_JPEG_QUALITY, 80, cv.IMWRITE_JPEG_OPTIMIZE, True])\n",
    "                keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "                if descriptors is not None:\n",
    "                    assert len(keypoints) == descriptors.shape[0]\n",
    "                    keypoints_list.append(keypoints)\n",
    "                    descriptors_list.append(descriptors)\n",
    "                    index_col_page.extend([page_i + 1] * descriptors.shape[0])\n",
    "                    index_col_type.extend([Page.unknown.value] * descriptors.shape[0])\n",
    "                    index_col_i.extend(list(range(descriptors.shape[0])))\n",
    "            else:  # Two pages per scanned image -> split it!\n",
    "                image_left, image_right = split_image(image)\n",
    "                assert cv.imwrite(f\"data/grey/{str(book_id)}/image/{str(book_id)}_{page_i+1:0>5}_{Page.left.value}.jpg\",\n",
    "                                  image_left, [cv.IMWRITE_JPEG_QUALITY, 80, cv.IMWRITE_JPEG_OPTIMIZE, True])\n",
    "                assert cv.imwrite(f\"data/grey/{str(book_id)}/image/{str(book_id)}_{page_i+1:0>5}_{Page.right.value}.jpg\",\n",
    "                                  image_right, [cv.IMWRITE_JPEG_QUALITY, 80, cv.IMWRITE_JPEG_OPTIMIZE, True])\n",
    "                keypoints_right, descriptors_right = detector.detectAndCompute(image_right, None)\n",
    "                if descriptors_right is not None:\n",
    "                    assert len(keypoints_right) == descriptors_right.shape[0]\n",
    "                    keypoints_list.append(keypoints_right)\n",
    "                    descriptors_list.append(descriptors_right)\n",
    "                    index_col_page.extend([page_i + 1] * descriptors_right.shape[0])\n",
    "                    index_col_type.extend([Page.right.value] * descriptors_right.shape[0])\n",
    "                    index_col_i.extend(list(range(descriptors_right.shape[0])))\n",
    "                keypoints_left, descriptors_left = detector.detectAndCompute(image_left, None)\n",
    "                if descriptors_left is not None:\n",
    "                    assert len(keypoints_left) == descriptors_left.shape[0]\n",
    "                    keypoints_list.append(keypoints_left)\n",
    "                    descriptors_list.append(descriptors_left)\n",
    "                    index_col_page.extend([page_i + 1] * descriptors_left.shape[0])\n",
    "                    index_col_type.extend([Page.left.value] * descriptors_left.shape[0])\n",
    "                    index_col_i.extend(list(range(descriptors_left.shape[0])))\n",
    "        index = pd.MultiIndex.from_arrays(\n",
    "            [index_col_page, index_col_type, index_col_i],\n",
    "            names=[\"page\", \"lr\", \"feature\"])\n",
    "        descriptors_df = pd.DataFrame(np.concatenate(descriptors_list), index=index)\n",
    "        descriptors_df.columns = descriptors_df.columns.map(str)\n",
    "        keypoints_df = pd.Series(chain.from_iterable(keypoints_list), index=index)\n",
    "        keypoints_df = pd.concat([\n",
    "            keypoints_df.apply(lambda x: x.pt[0]),\n",
    "            keypoints_df.apply(lambda x: x.pt[1]),\n",
    "            keypoints_df.apply(lambda x: x.size),\n",
    "            keypoints_df.apply(lambda x: x.angle),\n",
    "            keypoints_df.apply(lambda x: x.response),\n",
    "            keypoints_df.apply(lambda x: x.octave),\n",
    "            keypoints_df.apply(lambda x: x.class_id)\n",
    "        ], axis=1)\n",
    "        keypoints_df.columns = [\"x\", \"y\", \"size\", \"angle\", \"response\", \"octave\", \"class_id\"]\n",
    "        assert len(keypoints_df) == len(descriptors_df)\n",
    "        descriptors_df.to_parquet(f\"data/grey/{str(book_id)}/descriptors.parquet\", engine=\"pyarrow\")\n",
    "        keypoints_df.to_parquet(f\"data/grey/{str(book_id)}/keypoints.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matching features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_page_index(page_ids, nr_pages_df):\n",
    "    index = []\n",
    "    for book1_id, book2_id in combinations(page_ids, 2):\n",
    "        for page in range(1, nr_pages_df[book1_id] + 1):\n",
    "            index.append((book1_id, book2_id, page))\n",
    "    return pd.MultiIndex.from_tuples(index, names=(\"Book1\", \"Book2\", \"Page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_page_df(bukan_df, radius=8):\n",
    "    nr_pages = bukan_df[\"NrImages\"]\n",
    "    index = build_page_index(bukan_df.index, nr_pages)\n",
    "    diameter = radius * 2 + 1\n",
    "    page_array = np.empty((len(index), diameter), dtype=np.int32)\n",
    "    for i, (_, book2_id, page) in enumerate(index):\n",
    "        first_page = page - radius\n",
    "        last_page = page + radius\n",
    "        book2_nr_pages = nr_pages[book2_id]\n",
    "        def handle_borders(n):\n",
    "            if n < 0:\n",
    "                return 0\n",
    "            elif n > book2_nr_pages:\n",
    "                return 0\n",
    "            else:\n",
    "                return n\n",
    "        pages = np.fromiter((handle_borders(p) for p in range(first_page, last_page + 1)),\n",
    "                            count=diameter, dtype=np.int32)\n",
    "        page_array[i,:] = pages\n",
    "    columns = pd.RangeIndex(-radius, radius+1)\n",
    "    return pd.DataFrame(page_array, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors_df_to_series(descriptors_df, page_enum: Page):\n",
    "    descriptors_df = descriptors_df.swaplevel('lr', 'page').loc[page_enum.value]\n",
    "    pages = descriptors_df.index.get_level_values(0).unique()\n",
    "    return pd.Series([descriptors_df.loc[page].values for page in pages], index=pages)\n",
    "def read_descriptor_dict(book_ids, page_enum: Page):\n",
    "    desc_dict = {\n",
    "        book_id:descriptors_df_to_series(\n",
    "            pd.read_parquet(f\"data/grey/{str(book_id)}/descriptors.parquet\", engine=\"pyarrow\"),\n",
    "            page_enum)\n",
    "        for book_id in book_ids}\n",
    "    return desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_of_page_to_list(keypoints_of_page):\n",
    "    return [cv.KeyPoint(*args) for args in zip(*(column for _, column in keypoints_of_page.items()))]\n",
    "def keypoints_df_to_series(keypoints_df, page_enum: Page):\n",
    "    keypoints_df = keypoints_df.swaplevel('lr', 'page').loc[page_enum.value]\n",
    "    pages = keypoints_df.index.get_level_values(0).unique()\n",
    "    return pd.Series([keypoints_of_page_to_list(keypoints_df.loc[page]) for page in pages], index=pages)\n",
    "def read_keypoints_dict(book_ids, page_enum: Page):\n",
    "    kps_dict = {\n",
    "        book_id:keypoints_df_to_series(\n",
    "            pd.read_parquet(f\"data/grey/{str(book_id)}/keypoints.parquet\", engine=\"pyarrow\"),\n",
    "            page_enum)\n",
    "        for book_id in book_ids}\n",
    "    return kps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wradius = 200.\n",
    "hradius = 200.\n",
    "def is_near(pt1, pt2):\n",
    "    x1, y1 = pt1\n",
    "    x2, y2 = pt2\n",
    "    return ((x1 - wradius) <= x2 <= (x1 + wradius)) and ((y1 - hradius) <= y2 <= (y1 + hradius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_matches(row_index, nr_matches,\n",
    "                         filtered_masks_df, selected_keypoints_mask_df, matches_df, page_df):\n",
    "    book1_id, book2_id, book1_page, book2_offset = row_index\n",
    "    loc = (book1_id, book2_id, book1_page)\n",
    "    book2_page = page_df.loc[loc, book2_offset]\n",
    "    \n",
    "    filtered_mask = filtered_masks_df.loc[loc, book2_offset]\n",
    "    selected_keypoints_mask = selected_keypoints_mask_df.loc[loc, book2_offset]\n",
    "    assert selected_keypoints_mask.sum() == filtered_mask.size\n",
    "    \n",
    "    kp_mask = np.empty_like(selected_keypoints_mask, dtype=np.bool)\n",
    "    counter = 0\n",
    "    for i, kp_bool in enumerate(selected_keypoints_mask):\n",
    "        if kp_bool:\n",
    "            kp_mask[i] = filtered_mask[counter]\n",
    "            counter += 1\n",
    "        else:\n",
    "            kp_mask[i] = False\n",
    "    assert counter == selected_keypoints_mask.sum()\n",
    "    \n",
    "    matches = matches_df.loc[loc, book2_offset]\n",
    "    assert len(matches) == kp_mask.size\n",
    "    relevant_matches = [matches[i] for i, val in enumerate(kp_mask) if val]\n",
    "    assert len(relevant_matches) == kp_mask.sum()\n",
    "    \n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [[book1_id], [book1_page], [book2_id], [book2_page], pd.RangeIndex(len(relevant_matches))],\n",
    "        names=[\"book1\", \"page1\", \"book2\", \"page2\", \"match\"])\n",
    "    \n",
    "    assert nr_matches == len(relevant_matches)\n",
    "    return pd.DataFrame([(m.queryIdx, m.trainIdx, m.distance) for m in relevant_matches],\n",
    "                        columns=[\"queryIdx\", \"trainIdx\", \"distance\"], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pipeline_step(bukan_title, page_type: Page, matcher: cv.BFMatcher, final_threshold: int):\n",
    "    print(\"Processing:\", bukan_title)\n",
    "    basename = bukan_title + \"_\" + page_type.name\n",
    "    filename = basename + \".parquet\"\n",
    "    filepath = os.path.join(\"data/grey\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Results already exist:\", filepath, \"(Skipping)\")\n",
    "        return\n",
    "    subset = final_overview[final_overview[\"書名（統一書名）\"] == bukan_title]\n",
    "    subset = subset.sort_values(\"NrImages\", ascending=False)\n",
    "    subset_page_df = build_page_df(subset)\n",
    "    subset_page_df = subset_page_df[subset_page_df.sum(axis=1) != 0]\n",
    "    nr_combinations = subset_page_df.stack()[subset_page_df.stack() != 0].size\n",
    "    if nr_combinations > 2000000:\n",
    "        print(\"Too many page combinations:\", nr_combinations, \"Skipping\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Number of page combinations:\", nr_combinations)\n",
    "    try:\n",
    "        desc_dict = read_descriptor_dict(subset.index, page_type)\n",
    "        kps_dict = read_keypoints_dict(subset.index, page_type)\n",
    "    except KeyError:\n",
    "        print(\"No pages with this orientation found:\", page_type.name, \"(Skipping)\")\n",
    "        return\n",
    "    del subset\n",
    "    match_time_s, matches_df = applymapi(subset_page_df, find_matches,\n",
    "                                         max_distance=100, descriptors=desc_dict,\n",
    "                                         matcher=matcher)\n",
    "    del desc_dict\n",
    "    nr_machtes_total = matches_df.applymap(lambda x: len(x) if isinstance(x, Iterable) else 0).sum().sum()\n",
    "    print(\"Total number of matches found:\", nr_machtes_total)\n",
    "    select_time_s, selected_keypoints_df = applymapi(\n",
    "        subset_page_df, select_keypoints,\n",
    "        matches=matches_df, keypoints=kps_dict, filter=is_near)\n",
    "    del kps_dict\n",
    "    homography_time_s, (homography_df, hmask_df) = compute_homography_and_mask(\n",
    "        selected_keypoints_df.applymap(itemgetter(0)))\n",
    "    filter_time, filtered_masks_df = filter_out_bad_homographies(homography_df, hmask_df)\n",
    "    del homography_df\n",
    "    del hmask_df\n",
    "    hmask_sum_df = sum_homography_mask(filtered_masks_df)\n",
    "    flattened_sums = hmask_sum_df.stack().astype(np.int32)\n",
    "    thresholded_sums = flattened_sums[flattened_sums > final_threshold]\n",
    "    try:\n",
    "        relevant_matches_df = pd.concat([\n",
    "            get_relevant_matches(row_index, nr_matches, filtered_masks_df,\n",
    "                selected_keypoints_df.applymap(itemgetter(1)),\n",
    "                matches_df, subset_page_df)\n",
    "            for row_index, nr_matches in thresholded_sums.items()\n",
    "        ])\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(\"Skipping\")\n",
    "        return\n",
    "    relevant_matches_df.to_parquet(filepath, engine=\"pyarrow\")\n",
    "    print(\"Results written to:\", filename)\n",
    "    total_time = match_time_s + select_time_s + homography_time_s + filter_time\n",
    "    print(f\"Total time: {total_time} seconds ({total_time/60} minutes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(page_type: Page):\n",
    "    matcher = cv.BFMatcher_create(normType=cv.NORM_HAMMING)\n",
    "    final_threshold = 40\n",
    "    for bukan_title in log_progress(final_overview_by_title.index, every=1, size=len(final_overview_by_title.index)):\n",
    "        single_pipeline_step(bukan_title, page_type, matcher, final_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_page_combinations(bukan_title, page_type: Page):\n",
    "    subset = final_overview[final_overview[\"書名（統一書名）\"] == bukan_title]\n",
    "    subset = subset.sort_values(\"NrImages\", ascending=False)\n",
    "    subset_page_df = build_page_df(subset)\n",
    "    subset_page_df = subset_page_df[subset_page_df.sum(axis=1) != 0]\n",
    "    nr_combinations = subset_page_df.stack()[subset_page_df.stack() != 0].size\n",
    "    try:\n",
    "        desc_dict = read_descriptor_dict(subset.index, page_type)\n",
    "        kps_dict = read_keypoints_dict(subset.index, page_type)\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    return nr_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_combination_df():\n",
    "    results = {}\n",
    "    for page_type in [Page.right, Page.left, Page.unknown]:\n",
    "        results_per_title = {}\n",
    "        for bukan_title in final_overview_by_title.index:\n",
    "            results_per_title[bukan_title] = get_nr_page_combinations(bukan_title, page_type)\n",
    "        results[page_type.name] = results_per_title\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_df_paths = glob(\"data/grey/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_and_enum_from_path(path):\n",
    "    _, filename = os.path.split(path)\n",
    "    basename, _ = os.path.splitext(filename)\n",
    "    bukan_title, page_enum = basename.split(\"_\")\n",
    "    return bukan_title, Page[page_enum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_index(bukan_title, page_enum, matches_df):\n",
    "    \"\"\"Does work inline\"\"\"\n",
    "    index_df = matches_df.index.to_frame()\n",
    "    index_df.insert(0, \"lr\", page_enum.value)\n",
    "    index_df.insert(0, \"title\", bukan_title)\n",
    "    new_index = pd.MultiIndex.from_frame(index_df)\n",
    "    matches_df.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_dfs = []\n",
    "for match_df_path in all_match_df_paths:\n",
    "    bukan_title, page_enum = get_title_and_enum_from_path(match_df_path)\n",
    "    match_df = pd.read_parquet(match_df_path)\n",
    "    expand_index(bukan_title, page_enum, match_df)\n",
    "    all_match_dfs.append(match_df)\n",
    "all_match_dfs = pd.concat(all_match_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_dfs = all_match_dfs.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_book_ids = set(all_match_dfs.index.levels[2]) | set(all_match_dfs.index.levels[4])\n",
    "match_overview = overview.drop(\"Images\", axis=1)\n",
    "match_overview = match_overview[match_overview.index.isin(matched_book_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiragana_mapping = {\n",
    "    'ほんちょうぶかん': \"honchōbukan\",\n",
    "    'たいへいぶかん': \"taihenbukan\",\n",
    "    'たいへいぶかんたいぜん': \"taihenbukantaizen\",\n",
    "    'せいとうぶかん': \"seitōbukan\",\n",
    "    'げんろくぶかんたいぜん': \"genrokubukantaizen\",\n",
    "    'ほうえいぶかんたいせい': \"hōeibukantaisen\",\n",
    "    'ごりんぶかん': \"gorinbukan\",\n",
    "    'ほうえいぶかん': \"hōeibukan\",\n",
    "    'しょうふうぶかん': \"shōfūbukan\",\n",
    "    'しょうえんぶかん': \"shōenbukan\",\n",
    "    'しょうとくぶかん': \"shōtokubukan\",\n",
    "    'きょうほうぶかん': \"kyōhōbukan\",\n",
    "    'えいせいぶかん': \"eiseibukan\", \n",
    "    'げんぶんぶかん': \"genbunbukan\",\n",
    "    'かんぽうぶかん': \"kanpōbukan\",\n",
    "    'えんきょうぶかん': \"enkyōbukan\",\n",
    "    'かんえんぶかん': \"kanenbukan\",\n",
    "    'ゆうしぶかん': \"yūshibukan\",\n",
    "    'ほうりゃくぶかん': \"hōryakubukan\",\n",
    "    'たいせいぶかん': \"taiseibukan\",\n",
    "    'めいわぶかん': \"meiwabukan\",\n",
    "    'たいへいりゃくぶかん': \"taiheiryakubukan\",\n",
    "    'しゅうぎょくぶかん': \"shūgyokubukan\",\n",
    "    'あんえいぶかん': \"aneibukan\",\n",
    "    'しゅうちんぶかん': \"shūchinbukan\",\n",
    "    'てんめいぶかん': \"tenmeibukan\",\n",
    "    'まんじゅぶかん': \"manjubukan\",\n",
    "    'かんせいぶかん': \"kanseibukan\",\n",
    "    'きょうわぶかん': \"kyōwabukan\",\n",
    "    'ぶんかぶかん': \"bunkabukan\",\n",
    "    'ぶんせいぶかん': \"bunseibukan\",\n",
    "    'かまくらぶかん': \"kamakurabukan\",\n",
    "    'かいほうりゃくぶかん': \"kaihōryakubukan\", \n",
    "    'てんぽうぶかん': \"tenpōbukan\",\n",
    "    'おうにんぶかん': \"ōninbukan\",\n",
    "    'こうかぶかん': \"kōkabukan\",\n",
    "    'ばんせいぶかん': \"banseibukan\",\n",
    "    'かえいぶかん': \"kaeibukan\",\n",
    "    'しょうえいぶかん': \"shōeibukan\",\n",
    "    'あんせいぶかん': \"anseibukan\",\n",
    "    'ぶんきゅうぶかん': \"bunkyūbukan\",\n",
    "    'けいおうぶかん': \"keiōbukan\",\n",
    "    'おくにわけぶかん': \"okuniwakebukan\",\n",
    "    'かいほうおくにわけりゃくぶかん': \"kaihō okuniwake ryakubukan\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "     '鎌倉武鑑': 'かまくらぶかん',\n",
    "     '応仁武鑑': 'おうにんぶかん',\n",
    "     '本朝武鑑': 'ほんちょうぶかん',\n",
    "     '太平武鑑': 'たいへいぶかん',\n",
    "     '太平武鑑大全': 'たいへいぶかんたいぜん',\n",
    "     '正統武鑑': 'せいとうぶかん',\n",
    "     '元禄武鑑大全': 'げんろくぶかんたいぜん',\n",
    "     '宝永武鑑大成': 'ほうえいぶかんたいせい',\n",
    "     '御林武鑑': 'ごりんぶかん',\n",
    "     '宝永武鑑': 'ほうえいぶかん',\n",
    "     '正風武鑑': 'しょうふうぶかん',\n",
    "     '賞延武鑑': 'しょうえんぶかん',\n",
    "     '正徳武鑑': 'しょうとくぶかん',\n",
    "     '享保武鑑': 'きょうほうぶかん',\n",
    "     '永世武鑑': 'えいせいぶかん',\n",
    "     '元文武鑑': 'げんぶんぶかん',\n",
    "     '寛保武鑑': 'かんぽうぶかん',\n",
    "     '延享武鑑': 'えんきょうぶかん',\n",
    "     '寛延武鑑': 'かんえんぶかん',\n",
    "     '宝暦武鑑': 'ほうりゃくぶかん',\n",
    "     '大成武鑑': 'たいせいぶかん',\n",
    "     '明和武鑑': 'めいわぶかん',\n",
    "     '安永武鑑': 'あんえいぶかん',\n",
    "     '天明武鑑': 'てんめいぶかん',\n",
    "     '寛政武鑑': 'かんせいぶかん',\n",
    "     '享和武鑑': 'きょうわぶかん',\n",
    "     '文化武鑑': 'ぶんかぶかん',\n",
    "     '文政武鑑': 'ぶんせいぶかん',\n",
    "     '天保武鑑': 'てんぽうぶかん',\n",
    "     '弘化武鑑': 'こうかぶかん',\n",
    "     '嘉永武鑑': 'かえいぶかん',\n",
    "     '安政武鑑': 'あんせいぶかん',\n",
    "     '文久武鑑': 'ぶんきゅうぶかん',\n",
    "     '有司武鑑': 'ゆうしぶかん',\n",
    "     '太平略武鑑': 'たいへいりゃくぶかん',\n",
    "     '袖玉武鑑': 'しゅうぎょくぶかん',\n",
    "     '袖珍武鑑': 'しゅうちんぶかん',\n",
    "     '万寿武鑑': 'まんじゅぶかん',\n",
    "     '懐宝略武鑑': 'かいほうりゃくぶかん',\n",
    "     '万世武鑑': 'ばんせいぶかん',\n",
    "     '御国分武鑑': 'おくにわけぶかん',\n",
    "     '懐宝御国分略武鑑': 'かいほうおくにわけりゃくぶかん',\n",
    "     '昇栄武鑑': 'しょうえいぶかん',\n",
    "     '慶応武鑑': 'けいおうぶかん'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_overview = match_overview.assign(TitleHiragana=match_overview[\"書名（統一書名）\"].apply(lambda x: label_mapping[x]))\n",
    "match_overview = match_overview.assign(TitleRomanji=match_overview[\"TitleHiragana\"].apply(lambda x: hiragana_mapping[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_overview.index.name = \"BookID\"\n",
    "match_overview.columns = [\"Release\", \"Classification\", \"Title\", \"Type\", \"RequestID\", \"Publication\", \"Year\",\n",
    "                          \"Count\", \"Unit\", \"PagesPerScan\", \"Aspect\", \"Scans\", \"TitleHiragana\", \"TitleRomanji\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_title_overview = match_overview.groupby([\"Title\", \"TitleHiragana\", \"TitleRomanji\", \"Aspect\", \"PagesPerScan\"]).count()[\"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matches_and_keypoints():\n",
    "    new_matches = {}\n",
    "    for bukan_title in log_progress(all_match_dfs.index.get_level_values(0).unique(), every=1, name=\"Bukan Title\"):\n",
    "        match_by_title = all_match_dfs.loc[bukan_title]\n",
    "        match_book_ids = set(match_by_title.index.get_level_values(1)) | set(match_by_title.index.get_level_values(3))\n",
    "        match_book_kps = {book_id:pd.read_parquet(f\"data/grey/{book_id}/keypoints.parquet\") for book_id in match_book_ids}\n",
    "        enum_dict = {}\n",
    "        for page_enum_value in match_by_title.index.get_level_values(0).unique():\n",
    "            match_by_title_and_enum = match_by_title.loc[page_enum_value]\n",
    "            selected_matches_list = []\n",
    "            for index in log_progress(match_by_title_and_enum.index.droplevel(\"match\").unique(), every=100, name=\"Page matches\"):\n",
    "                (book1_id, book1_page, book2_id, book2_page) = index\n",
    "                selected_matches = match_by_title_and_enum.loc[index]\n",
    "                keypoints1 = match_book_kps[book1_id].loc[(book1_page, page_enum_value)]\n",
    "                src_kps1 = selected_matches[\"queryIdx\"].apply(lambda x: keypoints1.loc[x]).add_prefix(\"src_\")\n",
    "                keypoints2 = match_book_kps[book2_id].loc[(book2_page, page_enum_value)]\n",
    "                dst_kps2 = selected_matches[\"trainIdx\"].apply(lambda x: keypoints2.loc[x]).add_prefix(\"dst_\")\n",
    "                selected_matches = pd.concat([selected_matches, src_kps1, dst_kps2], sort=False, axis=1)\n",
    "                selected_matches.index = pd.MultiIndex.from_tuples([index + (match_id,) for match_id in selected_matches.index])\n",
    "                selected_matches_list.append(selected_matches)\n",
    "            enum_dict[page_enum_value] = pd.concat(selected_matches_list)\n",
    "        new_matches[bukan_title] = pd.concat(enum_dict)\n",
    "    new_matches = pd.concat(new_matches)\n",
    "    new_matches.index.names = all_match_dfs.index.names\n",
    "    new_matches = new_matches.astype({\n",
    "        'queryIdx': np.uint32,\n",
    "        'trainIdx': np.uint32,\n",
    "        'distance': np.float32,\n",
    "        'src_x': np.float32,\n",
    "        'src_y': np.float32,\n",
    "        'src_size': np.float32,\n",
    "        'src_angle': np.float32,\n",
    "        'src_response': np.float32,\n",
    "        'src_octave': np.uint8,\n",
    "        'src_class_id': np.uint8,\n",
    "        'dst_x': np.float32,\n",
    "        'dst_y': np.float32,\n",
    "        'dst_size': np.float32,\n",
    "        'dst_angle': np.float32,\n",
    "        'dst_response': np.float32,\n",
    "        'dst_octave': np.uint8,\n",
    "        'dst_class_id': np.uint8\n",
    "    })\n",
    "    return new_matches.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_matches.to_parquet(\"output/grey_matches.parquet.gzip\", engine=\"pyarrow\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match_overview.to_parquet(\"output/grey_overview.parquet.gzip\", engine=\"pyarrow\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>queryIdx</th>\n",
       "      <th>trainIdx</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th>lr</th>\n",
       "      <th>book1</th>\n",
       "      <th>page1</th>\n",
       "      <th>book2</th>\n",
       "      <th>page2</th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>文化武鑑</th>\n",
       "      <th>1</th>\n",
       "      <th>200018871</th>\n",
       "      <th>269</th>\n",
       "      <th>200018866</th>\n",
       "      <th>269</th>\n",
       "      <th>102</th>\n",
       "      <td>227</td>\n",
       "      <td>165</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有司武鑑</th>\n",
       "      <th>0</th>\n",
       "      <th>200019602</th>\n",
       "      <th>116</th>\n",
       "      <th>200019580</th>\n",
       "      <th>112</th>\n",
       "      <th>208</th>\n",
       "      <td>1113</td>\n",
       "      <td>629</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天保武鑑</th>\n",
       "      <th>2</th>\n",
       "      <th>200018923</th>\n",
       "      <th>422</th>\n",
       "      <th>200018921</th>\n",
       "      <th>420</th>\n",
       "      <th>18</th>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文政武鑑</th>\n",
       "      <th>2</th>\n",
       "      <th>200018896</th>\n",
       "      <th>431</th>\n",
       "      <th>200018887</th>\n",
       "      <th>430</th>\n",
       "      <th>82</th>\n",
       "      <td>219</td>\n",
       "      <td>157</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>有司武鑑</th>\n",
       "      <th>0</th>\n",
       "      <th>200019557</th>\n",
       "      <th>86</th>\n",
       "      <th>200019571</th>\n",
       "      <th>86</th>\n",
       "      <th>53</th>\n",
       "      <td>158</td>\n",
       "      <td>231</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                queryIdx  trainIdx  distance\n",
       "title lr book1     page1 book2     page2 match                              \n",
       "文化武鑑  1  200018871 269   200018866 269   102         227       165      47.0\n",
       "有司武鑑  0  200019602 116   200019580 112   208        1113       629      98.0\n",
       "天保武鑑  2  200018923 422   200018921 420   18           34        28      85.0\n",
       "文政武鑑  2  200018896 431   200018887 430   82          219       157      55.0\n",
       "有司武鑑  0  200019557 86    200019571 86    53          158       231      60.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_match_dfs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_src = cv.imread(\"data/grey/200018871/image/200018871_00269_1.jpg\", cv.IMREAD_GRAYSCALE)\n",
    "img_dst = cv.imread(\"data/grey/200018866/image/200018866_00269_1.jpg\", cv.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = new_matches.loc[(\"文化武鑑\", 1, 200018871, 269, 200018866, 269)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_kps = np.array([(x, y) for x, y in zip(match_data[\"src_x\"], match_data[\"src_y\"])])\n",
    "dst_kps = np.array([(x, y) for x, y in zip(match_data[\"dst_x\"], match_data[\"dst_y\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography, _ = cv.findHomography(src_kps, dst_kps, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_height, dst_width = img_dst.shape\n",
    "img_src_warped = cv.warpPerspective(img_src, homography, (dst_width, dst_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [\"blue\", \"green\", \"red\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in range(3):\n",
    "    bgr_src = cv.cvtColor(img_src_warped, cv.COLOR_GRAY2BGR)\n",
    "    bgr_dst = cv.cvtColor(img_dst, cv.COLOR_GRAY2BGR)\n",
    "    bgr_dst[:,:,channel] = bgr_src[:,:,channel]\n",
    "    assert cv.imwrite(f\"200018866_269_200018871_269_{channel_names[channel]}.png\", bgr_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in range(3):\n",
    "    bgr_src = cv.cvtColor(img_src_warped, cv.COLOR_GRAY2BGR)\n",
    "    bgr_dst = cv.cvtColor(img_dst, cv.COLOR_GRAY2BGR)\n",
    "    bgr_src[:,:,channel] = bgr_dst[:,:,channel]\n",
    "    assert cv.imwrite(f\"200018871_269_200018866_269_{channel_names[channel]}.png\", bgr_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cv.imwrite(\"200018866_269_200018871_269_src.png\", img_src_warped)\n",
    "assert cv.imwrite(\"200018866_269_200018871_269_dst.png\", img_dst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
