{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will do mostly the same as the `07-pipeline` but use some tricks to speed things up like multiprocessing as well as making sure memory does not overflow by processing the data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "import logging\n",
    "from typing import List\n",
    "from itertools import combinations\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"08-pipeline-multiprocessing.log\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_string = \"mysql://bukanuser@localhost/bukan?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_query(query: str):\n",
    "    engine = create_engine(engine_string, convert_unicode=True)\n",
    "    with engine.connect() as conn:\n",
    "        results = conn.execute(text(query)).fetchall()\n",
    "    engine.dispose()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    \"\"\"From <https://github.com/kuk/log-progress>\"\"\"\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.read_csv(\"bukan-overview-final.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I need to do now per image is:\n",
    "\n",
    "1. Read, greyscale and crop all images\n",
    "2. Split right/left page if necessary\n",
    "3. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(Enum):\n",
    "    \"\"\"Japanese reading order is from right to left.\"\"\"\n",
    "    whole = 0\n",
    "    right = 1\n",
    "    left  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    target_height = 660\n",
    "    target_width = 990\n",
    "    height, width = img.shape\n",
    "    x1 = (width - target_width) // 2\n",
    "    y1 = (height - target_height) // 2\n",
    "    x2 = x1 + target_width\n",
    "    y2 = y1 + target_height\n",
    "    return img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv.imread(path, flags=cv.IMREAD_REDUCED_GRAYSCALE_4)\n",
    "    img = crop_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(img):\n",
    "    height, width = img.shape\n",
    "    assert width == 990\n",
    "    half_width = width // 2\n",
    "    return img[:, :half_width], img[:, half_width:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_nr_from_path(path):\n",
    "    return int(path[-9:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(image: np.ndarray, book_id: int, page_nr: int, page_enum: Page):\n",
    "    path = f\"output/images/{book_id}/{book_id}_{page_nr:0>5}_{page_enum.value}.jpg\"\n",
    "    assert cv.imwrite(path, image, [cv.IMWRITE_JPEG_QUALITY, 80, cv.IMWRITE_JPEG_OPTIMIZE, True])\n",
    "    logging.info(f\"Image written: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors_to_dataframe(descriptors: np.ndarray, book_id: int, page_nr: int, page_enum: Page):\n",
    "    df = pd.DataFrame(descriptors)\n",
    "    df.index = pd.MultiIndex.from_product([[book_id], [page_nr], [page_enum.value], df.index],\n",
    "                                          names=[\"book\", \"page\", \"lr\", \"feature\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_to_dataframe(keypoints: List[cv.KeyPoint], book_id: int, page_nr: int, page_enum: Page):\n",
    "    df = pd.DataFrame([(kp.pt[0], kp.pt[1], kp.size, kp.angle, kp.response, kp.octave, kp.class_id) for kp in keypoints],\n",
    "                      columns=[\"x\", \"y\", \"size\", \"angle\", \"response\", \"octave\", \"class_id\"])\n",
    "    df.index = pd.MultiIndex.from_product([[book_id], [page_nr], [page_enum.value], df.index],\n",
    "                                          names=[\"book\", \"page\", \"lr\", \"feature\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(image: np.ndarray, book_id: int, page_nr: int, page_enum: Page,\n",
    "                    engine: sqlalchemy.engine.Engine, detector: cv.Feature2D):\n",
    "    keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        logging.warning(f\"No features detected for: {book_id}/{page_nr}/{page_enum.name}\")\n",
    "        return\n",
    "    descriptors = descriptors_to_dataframe(descriptors, book_id, page_nr, page_enum)\n",
    "    descriptors.to_sql(\"descriptor\", engine, if_exists=\"append\")\n",
    "    logging.info(f\"Descriptors written to database for: {book_id}/{page_nr}/{page_enum.name}\")\n",
    "    keypoints = keypoints_to_dataframe(keypoints, book_id, page_nr, page_enum)\n",
    "    keypoints.to_sql(\"keypoint\", engine, if_exists=\"append\")\n",
    "    logging.info(f\"Keypoints written to database for: {book_id}/{page_nr}/{page_enum.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(path, book_id: int, nr_pages_per_image: int, engine: sqlalchemy.engine.Engine,\n",
    "                 detector: cv.Feature2D):\n",
    "    page_nr = extract_page_nr_from_path(path)\n",
    "    image = read_image(path)\n",
    "    if nr_pages_per_image == 1:\n",
    "        write_image(image, book_id, page_nr, Page.whole)\n",
    "        detect_features(image, book_id, page_nr, Page.whole, engine, detector)\n",
    "    elif nr_pages_per_image == 2:\n",
    "        left_image, right_image = split_image(image)\n",
    "        write_image(right_image, book_id, page_nr, Page.right)\n",
    "        detect_features(right_image, book_id, page_nr, Page.right, engine, detector)\n",
    "        write_image(left_image, book_id, page_nr, Page.left)\n",
    "        detect_features(left_image, book_id, page_nr, Page.left, engine, detector)\n",
    "    else:\n",
    "        logging.warning(f\"Strange number of pages per image for {path}: {nr_pages_per_image} (Skipping)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_images_and_features(overview_df: pd.DataFrame, engine: sqlalchemy.engine.Engine,\n",
    "                                          detector: cv.Feature2D):\n",
    "    try:\n",
    "        for book_id, book_metadata in log_progress(overview_df.iterrows(), every=1, size=len(overview_df), name=\"Rows\"):\n",
    "            os.makedirs(f\"output/images/{str(book_id)}\", exist_ok=True)\n",
    "            nr_images = book_metadata[\"NrImages\"]\n",
    "            nr_pages_per_image = book_metadata[\"NrPages\"]\n",
    "            image_paths = glob(f\"data/{book_id}/image/*.jpg\")\n",
    "            assert len(image_paths) == nr_images\n",
    "            image_paths.sort()\n",
    "            for path in image_paths:\n",
    "                process_path(path, book_id, nr_pages_per_image, engine, detector)\n",
    "    except Exception as e:\n",
    "        logging.critical(str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine(engine_string)\n",
    "#akaze = cv.AKAZE_create(cv.AKAZE_DESCRIPTOR_MLDB_UPRIGHT, descriptor_size=0, threshold=0.005)\n",
    "#start_time = time.monotonic()\n",
    "#save_preprocessed_images_and_features(remaining, engine, akaze)\n",
    "#stop_time = time.monotonic()\n",
    "#engine.dispose()\n",
    "#print(\"All of this took:\", stop_time - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Feature Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to get all all book combinations as well as a fixed page offset. For each combination I need to run the full pipeline:\n",
    "\n",
    "1. Find matching features\n",
    "2. Filter features by their position\n",
    "3. Compute the homography\n",
    "4. Select features using the homography mask\n",
    "4. **Don't threshold the features**\n",
    "5. Save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string)\n",
    "page_sql = pd.read_sql(\"page\", engine, index_col=\"id\")\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_page_ids(page_df, overview, radius=8):\n",
    "    page_title = page_df.set_index(\"book\", append=True).swaplevel()\n",
    "    page_id_tuples_complete = []\n",
    "    for title in overview[\"書名（統一書名）\"].unique():\n",
    "        subset = overview[overview[\"書名（統一書名）\"] == title]\n",
    "        for book1_id, book2_id in combinations(subset.sort_values(\"NrImages\", ascending=False).index, 2):\n",
    "            book1_pages = page_title.loc[book1_id]\n",
    "            book2_pages = page_title.loc[book2_id]\n",
    "            book2_pages_invdict = {(page, lr):page_id for page_id, (page, lr) in book2_pages.iterrows()}\n",
    "            for page_id, (page, lr) in book1_pages.iterrows():\n",
    "                page_id_tuples= [(page_id, book2_pages_invdict[(page2, lr)])\n",
    "                                 for page2 in range(page-radius, page+radius+1)\n",
    "                                 if (page2, lr) in book2_pages_invdict]\n",
    "                page_id_tuples_complete.extend(page_id_tuples)\n",
    "    return pd.DataFrame(page_id_tuples_complete,\n",
    "                        index=pd.RangeIndex(1, len(page_id_tuples_complete)+1),\n",
    "                        columns=[\"page1\", \"page2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id_df = get_matched_page_ids(page_sql, overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_chunk = page_id_df.iloc[0:10]\n",
    "page_ids = set(page_chunk.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERY UNSAFE\n",
    "descriptors = pd.DataFrame(\n",
    "    run_sql_query(\"SELECT * FROM descriptor WHERE page_id IN (%s)\" % \",\".join(map(str, page_ids))),\n",
    "    columns=[\"page_id\", \"feature\", *range(61)]\n",
    ").set_index([\"page_id\", \"feature\"]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO UNSAFE\n",
    "keypoints = pd.DataFrame(\n",
    "    run_sql_query(\"SELECT page_id, feature, x, y FROM keypoint WHERE page_id IN (%s)\" % \",\".join(map(str, page_ids))),\n",
    "    columns=[\"page_id\", \"feature\", \"x\", \"y\"]\n",
    ").set_index([\"page_id\", \"feature\"]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_dict = {page_id:descriptors.loc[page_id].values for page_id in descriptors.index.get_level_values(\"page_id\").unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_dict = {page_id:keypoints.loc[page_id] for page_id in keypoints.index.get_level_values(\"page_id\").unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_match_ids(page_id_pair):\n",
    "    return 0\n",
    "    # 1. find_matches\n",
    "    # 2. select_keypoints\n",
    "    # 3. compute_homography\n",
    "    # 4. filter_bad_homographies\n",
    "    # 5. chose_relevant_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_chunk.apply(process_match_ids, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
