{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will do mostly the same as the `07-pipeline` but use some tricks to speed things up like multiprocessing as well as making sure memory does not overflow by processing the data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "import ipyparallel as ipp\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "import logging\n",
    "from typing import List\n",
    "from itertools import combinations, chain, compress\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"08-pipeline-multiprocessing.log\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_string = \"mysql://bukanuser@localhost/bukan?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_query(query: str):\n",
    "    engine = create_engine(engine_string, convert_unicode=True)\n",
    "    with engine.connect() as conn:\n",
    "        results = conn.execute(text(query)).fetchall()\n",
    "    engine.dispose()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    \"\"\"From <https://github.com/kuk/log-progress>\"\"\"\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.read_csv(\"bukan-overview-final.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I need to do now per image is:\n",
    "\n",
    "1. Read, greyscale and crop all images\n",
    "2. Split right/left page if necessary\n",
    "3. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(Enum):\n",
    "    \"\"\"Japanese reading order is from right to left.\"\"\"\n",
    "    whole = 0\n",
    "    right = 1\n",
    "    left  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    target_height = 660\n",
    "    target_width = 990\n",
    "    height, width = img.shape\n",
    "    x1 = (width - target_width) // 2\n",
    "    y1 = (height - target_height) // 2\n",
    "    x2 = x1 + target_width\n",
    "    y2 = y1 + target_height\n",
    "    return img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv.imread(path, flags=cv.IMREAD_REDUCED_GRAYSCALE_4)\n",
    "    img = crop_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(img):\n",
    "    height, width = img.shape\n",
    "    assert width == 990\n",
    "    half_width = width // 2\n",
    "    return img[:, :half_width], img[:, half_width:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_nr_from_path(path):\n",
    "    return int(path[-9:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(image: np.ndarray, book_id: int, page_nr: int, page_enum: Page):\n",
    "    path = f\"output/images/{book_id}/{book_id}_{page_nr:0>5}_{page_enum.value}.jpg\"\n",
    "    assert cv.imwrite(path, image, [cv.IMWRITE_JPEG_QUALITY, 80, cv.IMWRITE_JPEG_OPTIMIZE, True])\n",
    "    logging.info(f\"Image written: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors_to_dataframe(descriptors: np.ndarray, book_id: int, page_nr: int, page_enum: Page):\n",
    "    df = pd.DataFrame(descriptors)\n",
    "    df.index = pd.MultiIndex.from_product([[book_id], [page_nr], [page_enum.value], df.index],\n",
    "                                          names=[\"book\", \"page\", \"lr\", \"feature\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_to_dataframe(keypoints: List[cv.KeyPoint], book_id: int, page_nr: int, page_enum: Page):\n",
    "    df = pd.DataFrame([(kp.pt[0], kp.pt[1], kp.size, kp.angle, kp.response, kp.octave, kp.class_id) for kp in keypoints],\n",
    "                      columns=[\"x\", \"y\", \"size\", \"angle\", \"response\", \"octave\", \"class_id\"])\n",
    "    df.index = pd.MultiIndex.from_product([[book_id], [page_nr], [page_enum.value], df.index],\n",
    "                                          names=[\"book\", \"page\", \"lr\", \"feature\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(image: np.ndarray, book_id: int, page_nr: int, page_enum: Page,\n",
    "                    engine: sqlalchemy.engine.Engine, detector: cv.Feature2D):\n",
    "    keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        logging.warning(f\"No features detected for: {book_id}/{page_nr}/{page_enum.name}\")\n",
    "        return\n",
    "    descriptors = descriptors_to_dataframe(descriptors, book_id, page_nr, page_enum)\n",
    "    descriptors.to_sql(\"descriptor\", engine, if_exists=\"append\")\n",
    "    logging.info(f\"Descriptors written to database for: {book_id}/{page_nr}/{page_enum.name}\")\n",
    "    keypoints = keypoints_to_dataframe(keypoints, book_id, page_nr, page_enum)\n",
    "    keypoints.to_sql(\"keypoint\", engine, if_exists=\"append\")\n",
    "    logging.info(f\"Keypoints written to database for: {book_id}/{page_nr}/{page_enum.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(path, book_id: int, nr_pages_per_image: int, engine: sqlalchemy.engine.Engine,\n",
    "                 detector: cv.Feature2D):\n",
    "    page_nr = extract_page_nr_from_path(path)\n",
    "    image = read_image(path)\n",
    "    if nr_pages_per_image == 1:\n",
    "        write_image(image, book_id, page_nr, Page.whole)\n",
    "        detect_features(image, book_id, page_nr, Page.whole, engine, detector)\n",
    "    elif nr_pages_per_image == 2:\n",
    "        left_image, right_image = split_image(image)\n",
    "        write_image(right_image, book_id, page_nr, Page.right)\n",
    "        detect_features(right_image, book_id, page_nr, Page.right, engine, detector)\n",
    "        write_image(left_image, book_id, page_nr, Page.left)\n",
    "        detect_features(left_image, book_id, page_nr, Page.left, engine, detector)\n",
    "    else:\n",
    "        logging.warning(f\"Strange number of pages per image for {path}: {nr_pages_per_image} (Skipping)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_images_and_features(overview_df: pd.DataFrame, engine: sqlalchemy.engine.Engine,\n",
    "                                          detector: cv.Feature2D):\n",
    "    try:\n",
    "        for book_id, book_metadata in log_progress(overview_df.iterrows(), every=1, size=len(overview_df), name=\"Rows\"):\n",
    "            os.makedirs(f\"output/images/{str(book_id)}\", exist_ok=True)\n",
    "            nr_images = book_metadata[\"NrImages\"]\n",
    "            nr_pages_per_image = book_metadata[\"NrPages\"]\n",
    "            image_paths = glob(f\"data/{book_id}/image/*.jpg\")\n",
    "            assert len(image_paths) == nr_images\n",
    "            image_paths.sort()\n",
    "            for path in image_paths:\n",
    "                process_path(path, book_id, nr_pages_per_image, engine, detector)\n",
    "    except Exception as e:\n",
    "        logging.critical(str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine(engine_string)\n",
    "#akaze = cv.AKAZE_create(cv.AKAZE_DESCRIPTOR_MLDB_UPRIGHT, descriptor_size=0, threshold=0.005)\n",
    "#start_time = time.monotonic()\n",
    "#save_preprocessed_images_and_features(remaining, engine, akaze)\n",
    "#stop_time = time.monotonic()\n",
    "#engine.dispose()\n",
    "#print(\"All of this took:\", stop_time - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Feature Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to get all all book combinations as well as a fixed page offset. For each combination I need to run the full pipeline:\n",
    "\n",
    "1. Find matching features\n",
    "2. Filter features by their position\n",
    "3. Compute the homography\n",
    "4. Select features using the homography mask\n",
    "4. **Don't threshold the features**\n",
    "5. Save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(engine_string)\n",
    "page_sql = pd.read_sql(\"page\", engine, index_col=\"id\")\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_page_ids(page_df, overview, radius=8):\n",
    "    page_title = page_df.set_index(\"book\", append=True).swaplevel()\n",
    "    page_id_tuples_complete = []\n",
    "    for title in overview[\"書名（統一書名）\"].unique():\n",
    "        subset = overview[overview[\"書名（統一書名）\"] == title]\n",
    "        for book1_id, book2_id in combinations(subset.sort_values(\"NrImages\", ascending=False).index, 2):\n",
    "            book1_pages = page_title.loc[book1_id]\n",
    "            book2_pages = page_title.loc[book2_id]\n",
    "            book2_pages_invdict = {(page, lr):page_id for page_id, (page, lr) in book2_pages.iterrows()}\n",
    "            for page_id, (page, lr) in book1_pages.iterrows():\n",
    "                page_id_tuples= [(page_id, book2_pages_invdict[(page2, lr)])\n",
    "                                 for page2 in range(page-radius, page+radius+1)\n",
    "                                 if (page2, lr) in book2_pages_invdict]\n",
    "                page_id_tuples_complete.extend(page_id_tuples)\n",
    "    return pd.DataFrame(page_id_tuples_complete,\n",
    "                        index=pd.RangeIndex(1, len(page_id_tuples_complete)+1),\n",
    "                        columns=[\"page1\", \"page2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leyh/bukan-collection/venv/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"page_id.csv\"):\n",
    "    page_id_combinations_df = pd.read_csv(\"page_id.csv\", index_col=0)\n",
    "else:\n",
    "    page_id_combinations_df = get_matched_page_ids(page_sql, overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_match_id_batch(page_id_df, descriptors: dict, keypoints: dict):\n",
    "    matcher = cv.BFMatcher_create(normType=cv.NORM_HAMMING)\n",
    "    max_match_distance = 100\n",
    "    wradius = 100.\n",
    "    hradius = 100.\n",
    "    def is_near(points_pair):\n",
    "        (x1, y1), (x2, y2) = points_pair\n",
    "        return ((x1 - wradius) <= x2 <= (x1 + wradius)) and ((y1 - hradius) <= y2 <= (y1 + hradius))\n",
    "    def process_match_ids(page_id_pair):\n",
    "        left, right = page_id_pair\n",
    "        # 1. find_matches\n",
    "        left_desc = descriptors[left]\n",
    "        right_desc = descriptors[right]\n",
    "        matches = matcher.radiusMatch(left_desc, right_desc, max_match_distance)\n",
    "        matches = list(chain.from_iterable(matches))\n",
    "        if len(matches) == 0:\n",
    "            return None\n",
    "        # 2. select_keypoints\n",
    "        left_kps = keypoints[left]\n",
    "        right_kps = keypoints[right]\n",
    "        left_points = np.empty((len(matches), 2), dtype=np.float32)\n",
    "        right_points = np.empty((len(matches), 2), dtype=np.float32)\n",
    "        for i, match in enumerate(matches):\n",
    "            left_points[i, :] = left_kps.loc[match.queryIdx].values\n",
    "            right_points[i, :] = right_kps.loc[match.trainIdx].values\n",
    "        zipped_points = zip(left_points, right_points)\n",
    "        points_mask = np.fromiter(map(is_near, zipped_points), dtype=np.bool, count=len(matches))\n",
    "        left_points = left_points[points_mask]\n",
    "        right_points = right_points[points_mask]\n",
    "        assert left_points.shape == right_points.shape\n",
    "        if left_points.size == 0:\n",
    "            return None\n",
    "        # 3. compute_homography\n",
    "        h, h_mask = cv.findHomography(left_points, right_points, cv.RANSAC)\n",
    "        h_mask_sum = h_mask.sum()\n",
    "        if not h_mask_sum > 0:\n",
    "            return None\n",
    "        # 4. filter_bad_homographies\n",
    "        is_bad_h = np.any(np.isnan(h)) or np.any(np.absolute(h[2,:2]) > 0.001)\n",
    "        if is_bad_h:\n",
    "            return None\n",
    "        # 5. chose_relevant_matches\n",
    "        h_mask = np.squeeze(h_mask).astype(np.bool)\n",
    "        j = 0\n",
    "        for i in range(len(points_mask)):\n",
    "            if points_mask[i]:\n",
    "                if not h_mask[j]:\n",
    "                    points_mask[i] = False\n",
    "                j += 1\n",
    "        assert points_mask.sum() == h_mask.sum()\n",
    "        relevant_matches = compress(matches, points_mask)\n",
    "        relevant_pairs = ((match.queryIdx, match.trainIdx) for match in relevant_matches)\n",
    "        relevant_left, relevant_right = zip(*relevant_pairs)\n",
    "        return (relevant_left, relevant_right)\n",
    "    features = page_id_df.apply(process_match_ids, axis=1)\n",
    "    features = features.dropna()\n",
    "    result_df_list = []\n",
    "    for index, feature in features.items():\n",
    "        src_page_id, dst_page_id = page_id_df.loc[index]\n",
    "        src_page_ids = np.full(len(feature[0]), src_page_id, dtype=np.uint32)\n",
    "        src_features = np.array(feature[0], dtype=np.uint32)\n",
    "        dst_page_ids = np.full(len(feature[1]), dst_page_id, dtype=np.uint32)\n",
    "        dst_features = np.array(feature[1], dtype=np.uint32)\n",
    "        result_df_list.append(\n",
    "            pd.DataFrame([src_page_ids, src_features, dst_page_ids, dst_features], dtype=np.uint32,\n",
    "                         index=[\"src_page_id\", \"src_feature\", \"dst_page_id\", \"dst_feature\"]).T\n",
    "        )\n",
    "    return pd.concat(result_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_everything(chunksize, skip_multiplyer=0):\n",
    "    engine = create_engine(engine_string)\n",
    "    total_size = len(page_id_combinations_df)\n",
    "    for chunk_i in log_progress(\n",
    "            range(skip_multiplyer*chunksize, total_size, chunksize),\n",
    "            every=1, size=(total_size//chunksize)-skip_multiplyer):\n",
    "        page_chunk = page_id_combinations_df.iloc[chunk_i:chunk_i+chunksize]\n",
    "        page_ids = set(page_chunk.unstack())\n",
    "        try:\n",
    "            # VERY UNSAFE\n",
    "            descriptors = pd.DataFrame(\n",
    "                run_sql_query(\"SELECT * FROM descriptor WHERE page_id IN (%s)\" % \",\".join(map(str, page_ids))),\n",
    "                columns=[\"page_id\", \"feature\", *range(61)]\n",
    "            ).set_index([\"page_id\", \"feature\"]).astype(np.uint8)\n",
    "            descriptors = {page_id:descriptors.loc[page_id].values for page_id in descriptors.index.get_level_values(\"page_id\").unique()}\n",
    "            # ALSO UNSAFE\n",
    "            keypoints = pd.DataFrame(\n",
    "                run_sql_query(\"SELECT page_id, feature, x, y FROM keypoint WHERE page_id IN (%s)\" % \",\".join(map(str, page_ids))),\n",
    "                columns=[\"page_id\", \"feature\", \"x\", \"y\"]\n",
    "            ).set_index([\"page_id\", \"feature\"]).astype(np.float32)\n",
    "            keypoints = {page_id:keypoints.loc[page_id] for page_id in keypoints.index.get_level_values(\"page_id\").unique()}\n",
    "            chunk_results = process_match_id_batch(page_chunk, descriptors, keypoints)\n",
    "            chunk_results.to_sql(\"fmatch\", engine, if_exists=\"append\", index=False)\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failure processing: {page_chunk.index}\")\n",
    "            logging.critical(str(e))\n",
    "            raise e\n",
    "        logging.info(f\"Successfully processed: {page_chunk.index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
